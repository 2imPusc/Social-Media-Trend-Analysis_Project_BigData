import os
import time
import logging
import requests
import pandas as pd
import json
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import argparse

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    filename="youtube_crawler.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# Load API Key t·ª´ .env
load_dotenv()
API_KEY = os.getenv("YOUTUBE_API_KEY")
if not API_KEY:
    logging.error("API Key kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh.")
    raise ValueError("API Key kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh trong bi·∫øn m√¥i tr∆∞·ªùng YOUTUBE_API_KEY.")

# C·∫•u h√¨nh th∆∞ m·ª•c l∆∞u tr·ªØ
DATA_DIR = "Data"
os.makedirs(DATA_DIR, exist_ok=True)

# File l∆∞u danh s√°ch video ƒë√£ thu th·∫≠p
CRAWLED_VIDEO_IDS_FILE = os.path.join(DATA_DIR, "crawled_video_ids.json")
if os.path.exists(CRAWLED_VIDEO_IDS_FILE):
    with open(CRAWLED_VIDEO_IDS_FILE, "r") as f:
        crawled_video_ids = set(json.load(f))
else:
    crawled_video_ids = set()

# Kh·ªüi t·∫°o YouTube API client
youtube = build("youtube", "v3", developerKey=API_KEY)

# H√†m ki·ªÉm tra k·∫øt n·ªëi YouTube
def check_youtube_status():
    try:
        response = requests.get("https://www.youtube.com", timeout=10)
        return response.status_code == 200
    except requests.RequestException:
        return False

# L∆∞u d·ªØ li·ªáu v√†o CSV
def save_to_csv(data, data_type):
    if not data:
        logging.warning(f"Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u v√†o {data_type}.")
        return
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.join(DATA_DIR, f"{data_type}_{timestamp}.csv")
        pd.DataFrame(data).to_csv(filename, index=False, encoding="utf-8-sig")
        logging.info(f"ƒê√£ l∆∞u {len(data)} b·∫£n ghi v√†o {filename}")
        print(f"‚úÖ ƒê√£ l∆∞u {len(data)} b·∫£n ghi v√†o {filename}")
    except Exception as e:
        logging.error(f"L·ªói khi l∆∞u file CSV {data_type}: {e}")

# L·∫•y danh s√°ch video th·ªãnh h√†nh
def get_trending_videos(region_code="US", max_videos=50, category_cache=None):
    videos = []
    next_page_token = None

    while len(videos) < max_videos:
        try:
            request = youtube.videos().list(
                part="id,snippet,statistics,contentDetails",
                chart="mostPopular",
                regionCode=region_code,
                maxResults=min(50, max_videos - len(videos)),
                pageToken=next_page_token
            )
            response = request.execute()
            logging.info(f"Quota used: 1 (videos.list)")

            for item in response["items"]:
                video_id = item["id"]
                if video_id in crawled_video_ids:
                    logging.info(f"B·ªè qua video tr√πng l·∫∑p: {video_id}")
                    continue
                videos.append({
                    "content_id": video_id,
                    "platform": "youtube",
                    "title": item["snippet"]["title"],
                    "content": item["snippet"].get("description", ""),
                    "created_at": datetime.strptime(item["snippet"]["publishedAt"], "%Y-%m-%dT%H:%M:%SZ").strftime("%Y-%m-%d %H:%M:%S"),
                    "source_id": item["snippet"]["channelId"],
                    "source_name": item["snippet"]["channelTitle"],
                    "category_id": item["snippet"]["categoryId"],
                    "category_name": get_category_name(item["snippet"]["categoryId"], category_cache),
                    "tags": json.dumps(item["snippet"].get("tags", [])),
                    "views": int(item["statistics"].get("viewCount", 0)),
                    "score": int(item["statistics"].get("likeCount", 0)),
                    "comment_count": int(item["statistics"].get("commentCount", 0)),
                    "duration": item["contentDetails"]["duration"],
                    "upvote_ratio": 1.0,
                    "url": f"https://youtube.com/watch?v={video_id}",
                    "author": item["snippet"].get("channelTitle", "N/A")
                })

            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break
            time.sleep(1)
        except HttpError as e:
            logging.error(f"L·ªói khi l·∫•y video th·ªãnh h√†nh: {e}")
            break

    return videos

# L·∫•y t√™n danh m·ª•c video
def get_category_name(category_id, cache=None):
    if cache is None:
        cache = {}
    if category_id in cache:
        return cache[category_id]
    try:
        request = youtube.videoCategories().list(part="snippet", id=category_id)
        response = request.execute()
        logging.info(f"Quota used: 1 (videoCategories.list)")
        if response["items"]:
            category_name = response["items"][0]["snippet"]["title"]
            cache[category_id] = category_name
            return category_name
        return "Unknown"
    except HttpError as e:
        logging.error(f"L·ªói khi l·∫•y danh m·ª•c video {category_id}: {e}")
        return "Unknown"

# L·∫•y b√¨nh lu·∫≠n video
def get_video_comments(video_id, max_comments=100):
    comments = []
    next_page_token = None
    remaining_comments = max_comments
    try:
        while remaining_comments > 0:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=min(remaining_comments, 100),
                pageToken=next_page_token
            )
            response = request.execute()
            logging.info(f"Quota used: 1 (commentThreads.list)")

            for item in response["items"]:
                comment = item["snippet"]["topLevelComment"]["snippet"]
                comments.append({
                    "comment_id": f"yt_{video_id}_{hash(comment['textDisplay'])}",
                    "content_id": video_id,
                    "platform": "youtube",
                    "content": comment["textDisplay"],
                    "created_at": datetime.strptime(comment["publishedAt"], "%Y-%m-%dT%H:%M:%SZ").strftime("%Y-%m-%d %H:%M:%S"),
                    "score": comment["likeCount"],
                    "author": comment["authorDisplayName"],
                    "source_name": item["snippet"].get("channelTitle", "Unknown")
                })
            remaining_comments -= len(response["items"])
            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break
            time.sleep(1)
    except HttpError as e:
        logging.error(f"L·ªói khi l·∫•y b√¨nh lu·∫≠n video {video_id}: {e}")
    return comments

# X·ª≠ l√Ω t·ª´ng video
def process_video(video, category_cache, max_comments):
    video_id = video["content_id"]
    logging.info(f"ƒêang x·ª≠ l√Ω video {video_id}: {video['title']}")
    comments = get_video_comments(video_id, max_comments) if max_comments > 0 else []
    return video, comments

# H√†m ch√≠nh
def crawl_youtube_trending(region_code="US", max_videos=50, max_comments_per_video=100, max_workers=3):
    global crawled_video_ids
    content_data = []
    comment_data = []
    category_cache = {}

    print(f"üîç L·∫•y video th·ªãnh h√†nh t·∫°i khu v·ª±c {region_code}...")
    videos = get_trending_videos(region_code, max_videos, category_cache)
    if not videos:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y video th·ªãnh h√†nh.")
        return

    # X·ª≠ l√Ω ƒëa lu·ªìng
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(
            lambda video: process_video(video, category_cache, max_comments_per_video),
            videos
        ))

    for video, comments in results:
        content_data.append(video)
        comment_data.extend(comments)
        crawled_video_ids.add(video["content_id"])

    save_to_csv(content_data, "contents")
    save_to_csv(comment_data, "comments")

    with open(CRAWLED_VIDEO_IDS_FILE, "w") as f:
        json.dump(list(crawled_video_ids), f)

    print(f"‚úÖ Ho√†n t·∫•t. ƒê√£ thu th·∫≠p {len(content_data)} video v√† {len(comment_data)} b√¨nh lu·∫≠n.")

# Entry point
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Crawl video th·ªãnh h√†nh YouTube.")
    parser.add_argument("--region", default="US", help="M√£ qu·ªëc gia (v√≠ d·ª•: US, VN, JP)")
    parser.add_argument("--max-videos", type=int, default=20, help="S·ªë l∆∞·ª£ng video")
    parser.add_argument("--max-comments", type=int, default=100, help="S·ªë b√¨nh lu·∫≠n m·ªói video")
    parser.add_argument("--max-workers", type=int, default=3, help="S·ªë lu·ªìng x·ª≠ l√Ω song song")
    args = parser.parse_args()

    crawl_youtube_trending(
        region_code=args.region,
        max_videos=args.max_videos,
        max_comments_per_video=args.max_comments,
        max_workers=args.max_workers
    )
